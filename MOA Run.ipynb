{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c569165-2c72-40d3-8503-fe4ba4c9e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "Setting up agent 'egy' using model: asafaya/bert-base-arabic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba455039c27e41409b516db3f8398d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--asafaya--bert-base-arabic. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47193dee4214dbfa8a0fc4bdd119d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ea888fb42456ea795194a9103f2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/334k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34dd75407b944e29636a3cab9c38eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e7fec889e643b6a0170e3d000ddc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe321711ee4b4221ad29785b4115ca28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0c49a02e47413ab590fd3ded93b550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18256\\1691673056.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent 'egy' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Create a Dummy Dataset for Sentiment Analysis\n",
    "#    (Label: 1 = positive, 0 = negative)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "data_train = {\n",
    "    \"text\": [\n",
    "        \"Ø§Ù†Ø§ Ù…Ø¨Ø³ÙˆØ· Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©\",  # Egyptian positive\n",
    "        \"Ø§Ù†Ø§ Ø²Ø¹Ù„Ø§Ù† Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©\",  # Egyptian negative\n",
    "        \"Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø¬Ùˆ Ø¬Ù…ÙŠÙ„\",     # General positive\n",
    "        \"Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø¬Ùˆ Ù…ØªØ¹Ø¨\",     # General negative\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0],\n",
    "}\n",
    "\n",
    "data_test = {\n",
    "    \"text\": [\n",
    "        \"Ø§Ù†Ø§ ÙØ±Ø­Ø§Ù†\",   # Egyptian positive\n",
    "        \"Ø§Ù†Ø§ Ù…ÙƒØªØ¦Ø¨\",   # General negative\n",
    "    ],\n",
    "    \"label\": [1, 0],\n",
    "}\n",
    "\n",
    "# Create Hugging Face Datasets\n",
    "train_dataset = Dataset.from_dict(data_train)\n",
    "test_dataset = Dataset.from_dict(data_test)\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Define a Preprocessing Function\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    # Tokenize the text with truncation and padding to a fixed max length.\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=32)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Load Pre-trained Models (Agents) and Fine-tune Them\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Define model names.\n",
    "# Replace these with dialect-specific models if available.\n",
    "model_names = {\n",
    "    \"egy\": \"asafaya/bert-base-arabic\",         # Proxy for an Egyptian dialect model\n",
    "    \"ksa\": \"CAMeL-Lab/camelbert-mix\",            # Often used for dialectal Arabic (e.g., Saudi)\n",
    "    \"general\": \"UBC-NLP/MARBERT\",                # General Arabic model\n",
    "}\n",
    "\n",
    "agents = {}\n",
    "\n",
    "for key, model_name in model_names.items():\n",
    "    print(f\"\\nSetting up agent '{key}' using model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer and model (set num_labels=2 for binary classification)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    # Tokenize the datasets using the agent's tokenizer\n",
    "    tokenized_train = dataset[\"train\"].map(\n",
    "        lambda x: preprocess_function(x, tokenizer), batched=True\n",
    "    )\n",
    "    tokenized_test = dataset[\"test\"].map(\n",
    "        lambda x: preprocess_function(x, tokenizer), batched=True\n",
    "    )\n",
    "    \n",
    "    # Create a data collator that dynamically pads the inputs when batching\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Define a compute_metrics function for evaluation during training\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "        return {\"accuracy\": acc, \"f1\": f1}\n",
    "    \n",
    "    # Set training arguments (using very few epochs and small batch sizes for demo)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{key}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=5,\n",
    "        disable_tqdm=True,\n",
    "        logging_dir=f\"./logs_{key}\",\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer for the agent\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    ########################## Fine-tune:
    "    print(f\"Training agent '{key}' ...\")\n",
    "    trainer.train()\n",
    "    print(f\"Evaluating agent '{key}' ...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Results for agent '{key}': {eval_results}\")\n",
    "    \n",
    "    # Store the agent's components for later use\n",
    "    agents[key] = {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"data_collator\": data_collator,\n",
    "        \"trainer\": trainer,\n",
    "        \"tokenized_test\": tokenized_test,\n",
    "    }\n",
    "\n",
   
    "# 4. Defining Ensemble:
    
    "\n",
    "class MoAEnsemble:\n",
    "    def __init__(self, agents):\n",
    "        self.agents = agents\n",
    "\n",
    "    def predict(self, texts):\n",
    "        all_logits = None\n",
    "        num_agents = len(self.agents)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Loop over each agent to get predictions\n",
    "        for key, agent in self.agents.items():\n",
    "            tokenizer = agent[\"tokenizer\"]\n",
    "            model = agent[\"model\"]\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            model.to(device)\n",
    "            \n",
    "            # Tokenize the input texts\n",
    "            inputs = tokenizer(\n",
    "                texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=32\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            \n",
    "            if all_logits is None:\n",
    "                all_logits = logits\n",
    "            else:\n",
    "                all_logits += logits\n",
    "        \n",
    "        # Average the logits from all agents\n",
    "        avg_logits = all_logits / num_agents\n",
    "        predictions = np.argmax(avg_logits, axis=-1)\n",
    "        return predictions, avg_logits\n",
    "\n",
    "# Instantiate the MoA ensemble with our agents\n",
    "moa = MoAEnsemble(agents)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Evaluate the Ensemble on the Test Set\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "test_texts = dataset[\"test\"][\"text\"]\n",
    "true_labels = dataset[\"test\"][\"label\"]\n",
    "\n",
    "ensemble_predictions, ensemble_logits = moa.predict(test_texts)\n",
    "\n",
    "ensemble_acc = accuracy_score(true_labels, ensemble_predictions)\n",
    "ensemble_f1 = f1_score(true_labels, ensemble_predictions, average=\"weighted\")\n",
    "\n",
    "print(\"\\n--- Ensemble Evaluation ---\")\n",
    "print(\"Ensemble Accuracy: \", ensemble_acc)\n",
    "print(\"Ensemble F1 Score: \", ensemble_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa4c2ab-aff8-432f-914b-b965948dc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating outputs from proposer models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGYBERT: Logits: [[-0.0812344178557396, -0.05056487396359444]]\n",
      "MARBERT: Logits: [[0.26162290573120117, 0.05119345709681511]]\n",
      "AraBERTv2: Logits: [[0.1367824375629425, 0.14392822980880737]]\n",
      "Formatting inputs for aggregation...\n",
      "Aggregating outputs using AraT5...\n",
      "\n",
      "Final Aggregated Response:\n",
      "Ø§Ù„ØªÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ù‰ Ø¹Ù„Ù‰ Ø¹Ù„Ù‰ Ø¹Ù„Ù‰ Ø¹Ù„Ù‰ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ù‚\n"
     ]
    }
   ],
   "source": [
    "########################################### TESTING\n",
    "\n",
    "input_text = \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ø³ÙŠØ§Ø­ÙŠØ© Ø§Ù„Ø´Ù‡ÙŠØ±Ø© ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©ØŸ\"\n",
    "\n",
    "#### Proposer outputs here\n",
    "\n",
    "print(\"Generating outputs from proposer models...\")\n",
    "proposer_outputs = {}\n",
    "for name, (tokenizer, model) in proposers.items():\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    response = f\"Logits: {logits.tolist()}\"  # Placeholder for processing logits into meaningful text\n",
    "    proposer_outputs[name] = response\n",
    "    print(f\"{name}: {response}\")\n",
    "\n",
    "# Combining proposer\n",
    "print(\"Formatting inputs for aggregation...\")\n",
    "combined_input = \" | \".join([f\"{name}: {response}\" for name, response in proposer_outputs.items()])\n",
    "\n",
    "aggregator_input = f\"\"\"\n",
    "You have received responses from different models about the question:\n",
    "\"{input_text}\"\n",
    "\n",
    "Responses:\n",
    "{combined_input}\n",
    "\n",
    "Please provide a coherent, concise, and accurate summary of the responses above.\n",
    "\"\"\"\n",
    "inputs = aggregator_tokenizer(aggregator_input, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "#### Proposer outputs here\n",
    "print(\"Aggregating outputs using AraT5...\")\n",
    "with torch.no_grad():\n",
    "    generated_ids = aggregator_model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  # Set a reasonable max length\n",
    "        num_beams=3      # Use beam search for better quality\n",
    "    )\n",
    "aggregated_response = aggregator_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print final \n",
    "print(\"\\nFinal Aggregated Response:\")\n",
    "print(aggregated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2c32a-0d0f-43f5-83c0-1f5de00239f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
